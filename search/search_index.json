{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Ollama MCP Server (Python)","text":"<p>Supercharge your AI assistant with local LLM access</p> <p>A Python implementation of the Model Context Protocol (MCP) server that exposes Ollama SDK functionality as MCP tools, enabling seamless integration between your local LLM models and MCP-compatible applications like Windsurf and VS Code.</p> <p>This is a Python port of the TypeScript ollama-mcp project.</p>"},{"location":"#features","title":"Features","text":"<ul> <li> Ollama Cloud Support \u2014 Full integration with Ollama's cloud platform</li> <li> 8 Comprehensive Tools \u2014 Full access to Ollama's SDK functionality</li> <li> Hot-Swap Architecture \u2014 Automatic tool discovery with zero-config</li> <li> Type-Safe \u2014 Built with Pydantic models and type hints</li> <li> Minimal Dependencies \u2014 Lightweight and fast</li> <li> Drop-in Integration \u2014 Works with Windsurf, VS Code, and other MCP clients</li> <li> Web Search &amp; Fetch \u2014 Real-time web search and content extraction via Ollama Cloud (planned)</li> <li> Hybrid Mode \u2014 Use local and cloud models seamlessly in one server</li> </ul>"},{"location":"#why-python","title":"Why Python?","text":"<p>This Python implementation provides the same functionality as the TypeScript version but with:</p> <ul> <li>Python Native \u2014 No Node.js dependencies required</li> <li>Poetry Package Management \u2014 Modern Python dependency management</li> <li>Async/Await \u2014 Native Python async support</li> <li>Pydantic Models \u2014 Robust data validation and type safety</li> <li>Poetry Scripts \u2014 Easy installation and execution</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<p>Type in your MCP-compatible chat window:</p> <ul> <li>MCP Tool: ollama / ollama_chat \u2014 Use model llava and tell me a bed time story</li> <li>MCP Tool: ollama / ollama_chat \u2014 Use model gpt-oss and tell me a bed time story</li> </ul>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Installation Guide \u2014 Get up and running in minutes</li> <li>Available Tools \u2014 See all 8 MCP tools</li> <li>Windsurf Integration \u2014 Set up with Windsurf IDE</li> <li>VS Code Integration \u2014 Set up with VS Code</li> </ul>"},{"location":"SERVER_CONTROL/","title":"Server Control Guide","text":"<p>This guide explains how to start, stop, and manage the Ollama MCP Server.</p>"},{"location":"SERVER_CONTROL/#quick-start","title":"Quick Start","text":""},{"location":"SERVER_CONTROL/#method-1-using-the-control-script-recommended","title":"Method 1: Using the Control Script (Recommended)","text":"<p>The easiest way to manage the server:</p> <pre><code># Start the server\npython server_control.py start\n\n# Check server status\npython server_control.py status\n\n# Stop the server\npython server_control.py stop\n\n# Restart the server\npython server_control.py restart\n</code></pre>"},{"location":"SERVER_CONTROL/#method-2-direct-execution","title":"Method 2: Direct Execution","text":"<p>Run the server directly (blocks terminal):</p> <pre><code># Using Poetry\npy -m poetry run mcp-ollama-python\n\n# Or directly with Python\npython -m mcp_ollama_python\n\n# Stop with Ctrl+C\n</code></pre>"},{"location":"SERVER_CONTROL/#method-3-programmatic-control","title":"Method 3: Programmatic Control","text":"<p>Control the server from Python code:</p> <pre><code>from mcp_ollama_python.main import run, stop\nimport threading\n\n# Start server in background thread\nserver_thread = threading.Thread(target=run, daemon=True)\nserver_thread.start()\n\n# ... do other work ...\n\n# Stop the server\nstop()\n</code></pre>"},{"location":"SERVER_CONTROL/#server-lifecycle","title":"Server Lifecycle","text":""},{"location":"SERVER_CONTROL/#starting-the-server","title":"Starting the Server","text":"<p>When you start the server, it will: 1. Initialize the Ollama client connection 2. Discover and load all tools from the <code>/tools</code> directory 3. Set up signal handlers for graceful shutdown 4. Start listening for MCP client connections via stdio</p> <p>Output: <pre><code>Starting Ollama MCP Server...\nPress Ctrl+C to stop the server\nServer started successfully!\nWaiting for MCP client connections...\n</code></pre></p>"},{"location":"SERVER_CONTROL/#stopping-the-server","title":"Stopping the Server","text":"<p>The server supports graceful shutdown through multiple methods:</p> <p>Keyboard Interrupt (Ctrl+C): - Sends SIGINT signal - Server completes current operations - Closes Ollama client connections - Cleans up resources</p> <p>Signal Termination: - SIGTERM signal for graceful shutdown - Used by the control script and system managers</p> <p>Programmatic Stop: - Call <code>stop()</code> function from Python - Sets shutdown event to trigger cleanup</p>"},{"location":"SERVER_CONTROL/#server-status","title":"Server Status","text":"<p>Check if the server is running:</p> <pre><code>python server_control.py status\n</code></pre> <p>Output examples: - <code>\u2713 Server is running (PID: 12345)</code> - <code>\u2717 Server is not running</code></p>"},{"location":"SERVER_CONTROL/#integration-with-windsurf","title":"Integration with Windsurf","text":"<p>The server works seamlessly with Windsurf using stdio protocol.</p> <p>Windsurf Configuration:</p> <p>Add to your Windsurf MCP settings (<code>.windsurf/mcp_config.json</code>):</p> <pre><code>{\n  \"mcpServers\": {\n    \"ollama\": {\n      \"command\": \"py\",\n      \"args\": [\"-m\", \"poetry\", \"run\", \"mcp-ollama-python\"],\n      \"cwd\": \"C:\\myCode/gitHub/mcp-ollama-python\"\n    }\n  }\n}\n</code></pre> <p>How it works: 1. Windsurf launches the server as a subprocess 2. Server communicates via stdin/stdout 3. Windsurf sends MCP protocol messages 4. Server responds with tool results 5. When Windsurf closes, server shuts down automatically</p>"},{"location":"SERVER_CONTROL/#troubleshooting","title":"Troubleshooting","text":""},{"location":"SERVER_CONTROL/#server-wont-start","title":"Server Won't Start","text":"<p>Check if already running: <pre><code>python server_control.py status\n</code></pre></p> <p>Check Ollama connection: <pre><code># Verify Ollama is running\ncurl http://localhost:11434/api/tags\n</code></pre></p> <p>Check for port conflicts: The server uses stdio, not ports, so this shouldn't be an issue.</p>"},{"location":"SERVER_CONTROL/#server-wont-stop","title":"Server Won't Stop","text":"<p>Force stop: <pre><code># Find the process\nps aux | grep mcp-ollama-python\n\n# Kill it (replace PID)\nkill -9 &lt;PID&gt;\n\n# Or use the control script (includes force kill)\npython server_control.py stop\n</code></pre></p>"},{"location":"SERVER_CONTROL/#clean-up-stale-pid-file","title":"Clean Up Stale PID File","text":"<p>If the control script reports a running server but it's not actually running:</p> <pre><code># Remove the PID file\nrm .mcp_ollama_server.pid\n\n# Or on Windows\ndel .mcp_ollama_server.pid\n</code></pre>"},{"location":"SERVER_CONTROL/#advanced-usage","title":"Advanced Usage","text":""},{"location":"SERVER_CONTROL/#running-as-a-service","title":"Running as a Service","text":"<p>Linux (systemd):</p> <p>Create <code>/etc/systemd/system/mcp-ollama.service</code>:</p> <pre><code>[Unit]\nDescription=Ollama MCP Server\nAfter=network.target\n\n[Service]\nType=simple\nUser=your-user\nWorkingDirectory=/path/to/mcp-ollama-python\nExecStart=/usr/bin/python3 -m mcp_ollama_python\nRestart=on-failure\nRestartSec=5\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Enable and start: <pre><code>sudo systemctl enable mcp-ollama.service\nsudo systemctl start mcp-ollama.service\nsudo systemctl status mcp-ollama.service\n</code></pre></p> <p>Windows (NSSM):</p> <pre><code># Install NSSM (Non-Sucking Service Manager)\n# Download from https://nssm.cc/\n\n# Install service\nnssm install OllamaMCP \"C:\\Python\\python.exe\" \"-m mcp_ollama_python\"\nnssm set OllamaMCP AppDirectory \"C:\\myCode\\gitHub\\mcp-ollama-python\"\nnssm start OllamaMCP\n</code></pre>"},{"location":"SERVER_CONTROL/#custom-ollama-host","title":"Custom Ollama Host","text":"<p>Set environment variable before starting:</p> <pre><code># Linux/Mac\nexport OLLAMA_HOST=\"http://custom-host:11434\"\npython server_control.py start\n\n# Windows\nset OLLAMA_HOST=http://custom-host:11434\npython server_control.py start\n</code></pre>"},{"location":"SERVER_CONTROL/#logging","title":"Logging","text":"<p>Redirect output to log file:</p> <pre><code># Start with logging\npython -m mcp_ollama_python &gt; server.log 2&gt;&amp;1 &amp;\n\n# View logs\ntail -f server.log\n</code></pre>"},{"location":"SERVER_CONTROL/#server-signals","title":"Server Signals","text":"<p>The server responds to the following signals:</p> <ul> <li>SIGINT (Ctrl+C): Graceful shutdown</li> <li>SIGTERM: Graceful shutdown (used by system managers)</li> <li>SIGKILL: Force kill (not graceful, use as last resort)</li> </ul>"},{"location":"SERVER_CONTROL/#exit-codes","title":"Exit Codes","text":"<ul> <li><code>0</code>: Success</li> <li><code>1</code>: Error or server not running</li> <li><code>130</code>: Interrupted by user (Ctrl+C)</li> </ul>"},{"location":"SERVER_CONTROL/#see-also","title":"See Also","text":"<ul> <li>Home - Main documentation</li> <li>Architecture - Server architecture details</li> <li>Available Tools - List of MCP tools</li> </ul>"},{"location":"architecture/","title":"Architecture","text":""},{"location":"architecture/#hot-swap-autoloader","title":"Hot-Swap Autoloader","text":"<p>This server uses a hot-swap autoloader pattern for tool discovery:</p> <pre><code>src/mcp_ollama_python/\n\u251c\u2500\u2500 main.py              # Entry point\n\u251c\u2500\u2500 server.py            # MCP server creation\n\u251c\u2500\u2500 autoloader.py        # Dynamic tool discovery\n\u251c\u2500\u2500 ollama_client.py     # Ollama HTTP client\n\u251c\u2500\u2500 models.py            # Pydantic type definitions\n\u251c\u2500\u2500 response_formatter.py # Response formatting\n\u2514\u2500\u2500 tools/               # Tool implementations\n    \u251c\u2500\u2500 chat.py          # Each exports tool_definition\n    \u251c\u2500\u2500 generate.py\n    \u251c\u2500\u2500 embed.py\n    \u251c\u2500\u2500 list.py\n    \u251c\u2500\u2500 show.py\n    \u251c\u2500\u2500 pull.py\n    \u251c\u2500\u2500 delete.py\n    \u2514\u2500\u2500 ps.py\n</code></pre> <p>Key Benefits:</p> <ul> <li>Add new tools by dropping files in <code>src/mcp_ollama_python/tools/</code></li> <li>Zero server code changes required</li> <li>Each tool is independently testable</li> </ul>"},{"location":"architecture/#adding-a-new-tool","title":"Adding a New Tool","text":"<ol> <li>Create <code>src/mcp_ollama_python/tools/your_tool.py</code>:</li> </ol> <pre><code>from typing import Dict, Any\nfrom ..models import ToolDefinition, ResponseFormat\nfrom ..ollama_client import OllamaClient\nfrom ..response_formatter import format_response\n\nasync def your_tool_handler(\n    ollama: OllamaClient, args: Dict[str, Any], format: ResponseFormat\n) -&gt; str:\n    \"\"\"Your tool implementation\"\"\"\n    # Implementation here\n    return format_response({\"result\": \"success\"}, format)\n\n# Tool definition\ntool_definition = ToolDefinition(\n    name=\"ollama_your_tool\",\n    description=\"Your tool description\",\n    input_schema={\n        \"type\": \"object\",\n        \"properties\": {\n            \"param\": {\"type\": \"string\"}\n        },\n        \"required\": [\"param\"]\n    },\n)\n</code></pre> <ol> <li>Done! The autoloader discovers it automatically.</li> </ol>"},{"location":"architecture/#scripts-package","title":"Scripts Package","text":"<p>Management scripts live in <code>src/mcp_ollama_python/scripts/</code>:</p> <pre><code>src/mcp_ollama_python/scripts/\n\u251c\u2500\u2500 __init__.py          # Package marker\n\u251c\u2500\u2500 mcp_interactive.py   # Interactive menu manager\n\u2514\u2500\u2500 server_control.py    # CLI server control\n</code></pre> <p>These are exposed as Poetry entry points:</p> <ul> <li><code>mcp-interactive</code> \u2192 <code>mcp_ollama_python.scripts.mcp_interactive:main</code></li> <li><code>mcp-server-control</code> \u2192 <code>mcp_ollama_python.scripts.server_control:main</code></li> </ul>"},{"location":"configuration/","title":"Configuration","text":""},{"location":"configuration/#environment-variables","title":"Environment Variables","text":"Variable Default Description <code>OLLAMA_HOST</code> <code>http://127.0.0.1:11434</code> Ollama server endpoint <code>OLLAMA_API_KEY</code> \u2014 API key for Ollama Cloud (when implemented)"},{"location":"configuration/#custom-ollama-host","title":"Custom Ollama Host","text":"<pre><code>export OLLAMA_HOST=\"http://localhost:11434\"\npy -m poetry run mcp-ollama-python\n</code></pre>"},{"location":"configuration/#ollama-cloud-configuration-planned","title":"Ollama Cloud Configuration (Planned)","text":"<pre><code>export OLLAMA_HOST=\"https://ollama.com\"\nexport OLLAMA_API_KEY=\"your-ollama-cloud-api-key\"\npy -m poetry run mcp-ollama-python\n</code></pre>"},{"location":"configuration/#mcp-model-configuration","title":"MCP Model Configuration","text":"<p>The server exposes local Ollama models through MCP. Configure available models in <code>mcp.json</code>:</p> <p><code>mcp-ollama-python/mcp.json</code> <pre><code>{\n  \"capabilities\": {\n    \"models\": [\n      {\n        \"name\": \"gpt-oss\",\n        \"provider\": \"ollama\",\n        \"description\": \"Local Ollama GPT-OSS model served through MCP\",\n        \"maxTokens\": 4096\n      }\n    ]\n  }\n}\n</code></pre></p> <p>Model Configuration Options:</p> <ul> <li><code>name</code> \u2014 Model identifier used by MCP clients</li> <li><code>provider</code> \u2014 Always <code>\"ollama\"</code> for this server</li> <li><code>description</code> \u2014 Human-readable model description</li> <li><code>maxTokens</code> \u2014 Maximum context window size</li> </ul>"},{"location":"configuration/#multiple-models","title":"Multiple Models","text":"<p>You can expose multiple Ollama models through MCP:</p> <pre><code>{\n  \"capabilities\": {\n    \"models\": [\n      {\n        \"name\": \"gpt-oss\",\n        \"provider\": \"ollama\",\n        \"description\": \"Local Ollama GPT-OSS model\",\n        \"maxTokens\": 4096\n      },\n      {\n        \"name\": \"llama3.2\",\n        \"provider\": \"ollama\",\n        \"description\": \"Llama 3.2 model for general tasks\",\n        \"maxTokens\": 8192\n      },\n      {\n        \"name\": \"codellama\",\n        \"provider\": \"ollama\",\n        \"description\": \"Code Llama for programming tasks\",\n        \"maxTokens\": 16384\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"development/","title":"Development","text":"<p>This guide is for contributors who want to work on the <code>mcp-ollama-python</code> codebase itself. If you just want to use the server, see Installation.</p>"},{"location":"development/#dev-setup","title":"Dev Setup","text":"<pre><code># Clone and install with all dependency groups\ngit clone https://github.com/pblagoje/mcp-ollama-python.git\ncd mcp-ollama-python\npy -m poetry install\n\n# Run the server locally\npy -m poetry run mcp-ollama-python\n</code></pre>"},{"location":"development/#testing","title":"Testing","text":"<pre><code>py -m poetry run pytest\n</code></pre>"},{"location":"development/#code-quality","title":"Code Quality","text":"<pre><code># Format\npy -m poetry run black src/\n\n# Lint\npy -m poetry run flake8 src/\n\n# Pre-commit hooks (install once, runs on every commit)\npy -m poetry run pre-commit install\npy -m poetry run pre-commit run --all-files\n</code></pre>"},{"location":"development/#building-the-windows-executable","title":"Building the Windows Executable","text":"<pre><code>poetry run pyinstaller mcp-ollama-python.spec --clean --distpath bin\n</code></pre> <p>The spec file reads the version from <code>pyproject.toml</code> and produces an EXE named like <code>mcp-ollama-python-1.0.3-win11-x64.exe</code>.</p>"},{"location":"development/#building-the-docs","title":"Building the Docs","text":"<pre><code># Install docs dependencies\npy -m poetry install --with docs\n\n# Live preview\npy -m poetry run mkdocs serve\n\n# Build static site\npy -m poetry run mkdocs build --strict\n</code></pre> <p>Docs are auto-deployed to GitHub Pages on push to <code>main</code> via the <code>docs.yml</code> workflow.</p>"},{"location":"development/#contributing","title":"Contributing","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch (<code>git checkout -b feature/amazing-feature</code>)</li> <li>Write tests for your changes</li> <li>Commit with clear messages (<code>git commit -m 'Add amazing feature'</code>)</li> <li>Push to your branch (<code>git push origin feature/amazing-feature</code>)</li> <li>Open a Pull Request</li> </ol>"},{"location":"development/#code-quality-standards","title":"Code Quality Standards","text":"<ul> <li>All new tools must export <code>tool_definition</code></li> <li>Maintain comprehensive test coverage</li> <li>Follow existing Python patterns (Black formatting, Pydantic schemas)</li> <li>See Architecture for how to add new tools</li> </ul>"},{"location":"development/#related-projects","title":"Related Projects","text":"<ul> <li>ollama-mcp (TypeScript) \u2014 Original TypeScript implementation</li> <li>Ollama \u2014 Get up and running with large language models locally</li> <li>Model Context Protocol \u2014 Open standard for AI assistant integration</li> <li>Windsurf \u2014 AI-powered code editor with MCP support</li> <li>Cline \u2014 VS Code AI assistant</li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>Ollama running locally \u2014 Download Ollama</li> </ul>"},{"location":"installation/#install-from-pypi","title":"Install from PyPI","text":"<pre><code>pip install mcp-ollama-python\n</code></pre> <p>That's it. Your MCP client (Windsurf, VS Code, etc.) will start the server automatically \u2014 you don't need to run it manually.</p>"},{"location":"installation/#configure-your-ide","title":"Configure Your IDE","text":""},{"location":"installation/#windsurf","title":"Windsurf","text":"<p>Add to <code>%USERPROFILE%\\.codeium\\windsurf\\mcp_config.json</code> (Windows) or <code>~/.codeium/windsurf/mcp_config.json</code> (macOS/Linux):</p> <pre><code>{\n  \"mcpServers\": {\n    \"ollama\": {\n      \"command\": \"py\",\n      \"args\": [\"-m\", \"mcp_ollama_python\"],\n      \"disabled\": false\n    }\n  }\n}\n</code></pre> <p>Restart Windsurf \u2014 the Ollama MCP server will appear in the MCP panel.</p> <p>See the full Windsurf Integration guide for advanced setup.</p>"},{"location":"installation/#vs-code","title":"VS Code","text":"<p>Add to your MCP settings:</p> <pre><code>{\n  \"mcpServers\": {\n    \"ollama\": {\n      \"command\": \"py\",\n      \"args\": [\"-m\", \"mcp_ollama_python\"],\n      \"disabled\": false\n    }\n  }\n}\n</code></pre> <p>See the full VS Code Integration guide for details.</p>"},{"location":"installation/#windows-executable","title":"Windows Executable","text":"<p>If you prefer a standalone <code>.exe</code> (no Python required), download it from the Releases page.</p>"},{"location":"installation/#verify-installation","title":"Verify Installation","text":"<pre><code># Check Ollama is running\ncurl http://localhost:11434/api/tags\n\n# Verify the module is installed\npy -m mcp_ollama_python --help\n</code></pre>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration \u2014 Environment variables, custom hosts, model config</li> <li>Available Tools \u2014 All 8 MCP tools with examples</li> </ul>"},{"location":"mcp_interactive/","title":"MCP Interactive Manager Documentation","text":""},{"location":"mcp_interactive/#overview","title":"Overview","text":"<p><code>mcp_interactive.py</code> is an interactive menu-driven Python script that provides comprehensive management and interaction capabilities for the Ollama MCP (Model Context Protocol) Server. It offers a user-friendly terminal interface for server lifecycle management, command execution, environment configuration, and monitoring.</p>"},{"location":"mcp_interactive/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Features</li> <li>Prerequisites</li> <li>Installation</li> <li>Quick Start</li> <li>Main Menu Options</li> <li>File Structure</li> <li>Configuration</li> <li>Usage Examples</li> <li>Technical Details</li> <li>Troubleshooting</li> </ul>"},{"location":"mcp_interactive/#features","title":"Features","text":""},{"location":"mcp_interactive/#core-capabilities","title":"Core Capabilities","text":"<ul> <li>Server Lifecycle Management: Start, stop, and monitor MCP server processes</li> <li>Process Validation: Intelligent PID file management with process verification using psutil</li> <li>Environment Management: Persistent environment variable configuration</li> <li>Interactive Command Execution: Execute MCP commands with guided parameter input</li> <li>Real-time Monitoring: Server status, Ollama connection, and available models display</li> <li>Comprehensive Logging: Separate log files for standard output and errors</li> <li>Cross-Platform Support: Windows and Unix/Linux compatibility</li> <li>Automatic Cleanup: Stale pipe file and PID file management</li> </ul>"},{"location":"mcp_interactive/#platform-specific-features","title":"Platform-Specific Features","text":"<ul> <li>Windows: Uses <code>psutil.Process.terminate()</code> instead of SIGKILL for graceful shutdown</li> <li>Unix/Linux: Standard signal-based process management</li> <li>Cross-platform: Automatic detection and appropriate handling</li> </ul>"},{"location":"mcp_interactive/#prerequisites","title":"Prerequisites","text":""},{"location":"mcp_interactive/#software","title":"Software","text":"Tool Minimum Version Why it's needed Python <code>3.10</code> or newer Project code targets Python 3.10+. Poetry <code>&gt;=1.6</code> Handles the project's virtual environment and dependencies. Ollama Latest stable Required for the <code>mcp_ollama_python</code> integration. <p>Tip: If you don't have Poetry installed, run <pre><code>curl -sSL https://install.python-poetry.org | python3 -\n</code></pre> or use your system package manager (e.g. <code>brew install poetry</code> on macOS, or <code>pip install poetry</code> on Windows).</p>"},{"location":"mcp_interactive/#python-packages","title":"Python Packages","text":"<p>The project's dependencies are declared in <code>pyproject.toml</code>:</p> <ul> <li><code>httpx &gt;=0.27.0</code></li> <li><code>pydantic &gt;=2.7.0</code></li> <li><code>rich &gt;=13.7.0</code></li> <li><code>mcp &gt;=1.0.0</code></li> <li><code>psutil &gt;=7.1.3</code></li> </ul> <p>No <code>requirements.txt</code> is needed \u2013 Poetry takes care of resolving and pinning the exact versions.</p>"},{"location":"mcp_interactive/#installation","title":"Installation","text":"<pre><code># 1. Clone the repository (if you haven't already)\ngit clone https://github.com/your-repo/mcp-ollama-python.git\ncd mcp-ollama-python\n\n# 2. (Optional) Verify that you are on the right Python version\npython --version   # Should print 3.10.x or newer\n\n# 3. Install the project's dependencies and create a virtual environment\npoetry install\n\n# 4. (Optional) Activate the virtual environment manually\npoetry shell\n#   \u2022 You'll now be inside the Poetry-managed venv and can run commands normally.\n\n# 5. Verify Ollama is running\n# Default: http://127.0.0.1:11434\n# Or custom host via OLLAMA_HOST environment variable\n</code></pre>"},{"location":"mcp_interactive/#what-poetry-install-does","title":"What <code>poetry install</code> does","text":"<ol> <li>Creates a fresh virtual environment (if one does not already exist).</li> <li>Installs the exact dependency versions defined in <code>pyproject.toml</code>.</li> <li>Sets up a <code>poetry.lock</code> file to guarantee reproducible installs.</li> </ol> <p>NOTE: The <code>poetry run</code> command automatically activates the environment for a single command, so you can skip <code>poetry shell</code> if you prefer a one-liner.</p>"},{"location":"mcp_interactive/#quick-start","title":"Quick Start","text":""},{"location":"mcp_interactive/#running-the-script","title":"Running the Script","text":"<p>The recommended way to run the script is via Poetry. Choose one of the two approaches below:</p> <p>Option 1 \u2013 Run directly with Poetry</p> <pre><code>poetry run python scripts/mcp_interactive.py\n</code></pre> <p>Option 2 \u2013 Activate the Poetry shell first</p> <pre><code>poetry shell\npython scripts/mcp_interactive.py\n</code></pre> <p>Both commands start the virtual environment automatically and execute the <code>mcp_interactive.py</code> script from the <code>scripts</code> directory.</p>"},{"location":"mcp_interactive/#basic-workflow","title":"Basic Workflow","text":"<ol> <li>Check Status (Option 1): Verify Ollama connection and available models</li> <li>Start Server (Option 2): Launch the MCP server in the background</li> <li>Run Commands (Option 8): Execute MCP tools interactively</li> <li>View Logs (Option 4): Monitor server output and errors</li> <li>Stop Server (Option 3): Gracefully shutdown the server</li> </ol>"},{"location":"mcp_interactive/#main-menu-options","title":"Main Menu Options","text":"<p>The interactive menu provides 9 main options:</p>"},{"location":"mcp_interactive/#1-check-mcp-server-status","title":"1. Check MCP Server Status","text":"<p>Displays comprehensive server and Ollama connection information:</p> <ul> <li>Server Status: Running/Not Running with PID</li> <li>PID File Location: Path to the PID file in <code>tmp/</code> directory</li> <li>Ollama Connection: Host URL and connection status</li> <li>Available Models: Count and list of first 5 models</li> </ul> <p>Example Output: <pre><code>============================================================\nSERVER STATUS\n============================================================\n\u2713 Server is RUNNING (PID: 12345)\n  PID File: C:\\myCode\\gitHub\\mcp-ollama-python\\tmp\\.mcp_ollama_server.pid\n\nOllama Connection:\n  Host: http://127.0.0.1:11434\n  Status: \u2713 Connected\n  Available Models: 21\n  Models: llama3.1:latest, qwen3-vl:latest, glm4:latest, phi4:latest, llava:latest\n           ... and 16 more\n============================================================\n</code></pre></p>"},{"location":"mcp_interactive/#2-start-server","title":"2. Start Server","text":"<p>Launches the MCP server as a background process with:</p> <ul> <li>Automatic Cleanup: Removes stale pipe files before starting</li> <li>Environment Variables: Applies custom environment configuration</li> <li>Logging: Redirects stdout/stderr to log files</li> <li>Process Isolation: Uses subprocess with proper signal handling</li> <li>PID Management: Creates PID file for process tracking</li> </ul> <p>Features: - Prevents duplicate server instances - Creates pipe file descriptors for graceful shutdown - Windows-specific process group creation - Automatic log file creation in <code>logs/</code> directory</p>"},{"location":"mcp_interactive/#3-stop-server","title":"3. Stop Server","text":"<p>Gracefully stops the running MCP server:</p> <ul> <li>Pipe Closure: Closes pipe file descriptors to signal shutdown</li> <li>SIGTERM: Sends termination signal for graceful exit</li> <li>Force Shutdown: Uses platform-specific force kill if needed (5-second timeout)</li> <li>Cleanup: Removes PID files and stale pipe files</li> <li>Verification: Confirms process termination</li> </ul> <p>Shutdown Process: 1. Close pipe file descriptor 2. Send SIGTERM signal 3. Wait up to 5 seconds for graceful shutdown 4. Force terminate if still running (Windows: psutil, Unix: SIGKILL) 5. Clean up all temporary files</p>"},{"location":"mcp_interactive/#4-view-server-logs","title":"4. View Server Logs","text":"<p>Displays server log files with detailed information:</p> <ul> <li>Standard Output Log: <code>logs/mcp_ollama_server.log</code></li> <li>Error Log: <code>logs/mcp_ollama_server_error.log</code></li> <li>File Sizes: Shows log file sizes for debugging</li> <li>UTF-8 Encoding: Handles encoding issues gracefully</li> </ul> <p>Features: - Distinguishes between empty and missing log files - Shows file paths for manual inspection - Error handling for file read operations</p>"},{"location":"mcp_interactive/#5-list-server-commands-and-arguments","title":"5. List Server Commands and Arguments","text":"<p>Discovers and displays all available MCP tools:</p> <ul> <li>Tool Discovery: Uses autoloader to find all registered tools</li> <li>Detailed Information: Shows name, description, and arguments</li> <li>Argument Details: Displays type, requirement status, and descriptions</li> <li>Schema Inspection: Shows complete input schema for each tool</li> </ul> <p>Example Output: <pre><code>Found 8 tools:\n\n1. ollama_list\n   Description: List all available Ollama models\n   Arguments:\n     * format (string): Output format (default: json)\n\n2. ollama_chat\n   Description: Interactive chat with Ollama models\n   Arguments:\n     * model (string): Name of the model to use\n     * messages (array): Array of chat messages\n       format (string): Output format (default: json)\n</code></pre></p>"},{"location":"mcp_interactive/#6-manage-environment-variables","title":"6. Manage Environment Variables","text":"<p>Submenu for comprehensive environment variable management:</p> <p>Sub-options: 1. View current environment variables: Display custom and system variables 2. Add/Update environment variable: Set or modify variables 3. Remove environment variable: Delete custom variables 4. Reset to defaults: Clear all custom variables 5. Back to main menu: Return to main menu</p> <p>Common Variables: - <code>OLLAMA_HOST</code>: Ollama server URL (default: http://127.0.0.1:11434) - <code>OLLAMA_API_KEY</code>: API key for Ollama (if required) - <code>OLLAMA_MODELS</code>: Custom models directory</p> <p>Persistence: Variables are saved to <code>tmp/.mcp_env_vars.json</code></p>"},{"location":"mcp_interactive/#7-view-current-environment-variables","title":"7. View Current Environment Variables","text":"<p>Displays environment configuration:</p> <ul> <li>Custom Variables: Variables set through the script</li> <li>System Variables: Ollama-related system environment variables</li> <li>Suggestions: Common variables you might want to set</li> </ul>"},{"location":"mcp_interactive/#8-run-mcp-command","title":"8. Run MCP Command","text":"<p>Interactive command execution with guided parameter input:</p> <p>Workflow: 1. Tool Selection: Choose from available MCP tools 2. Parameter Input: Guided prompts for each required/optional parameter 3. Format Selection: Choose JSON or Markdown output 4. Execution: Run the command with provided parameters 5. Result Display: Show formatted output</p> <p>Special Handling: - Chat Messages: Simplified input for chat message arrays - Arrays: Comma-separated value input - Objects: JSON format input with validation - Required vs Optional: Clear indicators for parameter requirements</p> <p>Example Session: <pre><code>Available commands (8):\n\n1. ollama_list\n   List all available Ollama models\n\nSelect command number: 1\n\nArguments:\n  format (string) [OPTIONAL]\n  Output format (default: json)\n  Enter value: json\n\nOutput format:\n  1. JSON\n  2. Markdown\nSelect format (1 or 2, default: 1): 1\n\n============================================================\nEXECUTING COMMAND...\n============================================================\n\nRESULT:\n{\n  \"models\": [...]\n}\n\n\u2713 Command executed successfully.\n</code></pre></p>"},{"location":"mcp_interactive/#9-exit","title":"9. Exit","text":"<p>Cleanly exits the application.</p>"},{"location":"mcp_interactive/#file-structure","title":"File Structure","text":"<p>The script manages files in organized directories:</p> <pre><code>mcp-ollama-python/\n\u251c\u2500\u2500 scripts/\n\u2502   \u2514\u2500\u2500 mcp_interactive.py          # Main script\n\u251c\u2500\u2500 tmp/                             # Temporary files (auto-created)\n\u2502   \u251c\u2500\u2500 .mcp_ollama_server.pid      # Server process ID\n\u2502   \u251c\u2500\u2500 .mcp_ollama_server_*.pipe   # Pipe file descriptors\n\u2502   \u2514\u2500\u2500 .mcp_env_vars.json          # Persistent environment variables\n\u251c\u2500\u2500 logs/                            # Log files (auto-created)\n\u2502   \u251c\u2500\u2500 mcp_ollama_server.log       # Standard output\n\u2502   \u2514\u2500\u2500 mcp_ollama_server_error.log # Error output\n\u2514\u2500\u2500 src/\n    \u2514\u2500\u2500 mcp_ollama_python/           # MCP server package\n</code></pre>"},{"location":"mcp_interactive/#directory-management","title":"Directory Management","text":"<ul> <li>Automatic Creation: <code>tmp/</code> and <code>logs/</code> directories are created automatically</li> <li>Cleanup: Stale pipe files are automatically removed</li> <li>Persistence: Environment variables persist across sessions</li> </ul>"},{"location":"mcp_interactive/#configuration","title":"Configuration","text":""},{"location":"mcp_interactive/#environment-variables","title":"Environment Variables","text":"<p>Set custom environment variables through the menu (Option 6) or directly in <code>tmp/.mcp_env_vars.json</code>:</p> <pre><code>{\n  \"OLLAMA_HOST\": \"http://ai:11434/\",\n  \"OLLAMA_API_KEY\": \"your-api-key-here\"\n}\n</code></pre>"},{"location":"mcp_interactive/#default-configuration","title":"Default Configuration","text":"<ul> <li>Ollama Host: <code>http://127.0.0.1:11434</code> (or <code>OLLAMA_HOST</code> env var)</li> <li>Log Directory: <code>logs/</code> (relative to project root)</li> <li>Temp Directory: <code>tmp/</code> (relative to project root)</li> <li>Server Timeout: 5 seconds for graceful shutdown</li> </ul>"},{"location":"mcp_interactive/#usage-examples","title":"Usage Examples","text":""},{"location":"mcp_interactive/#example-1-start-server-and-check-status","title":"Example 1: Start Server and Check Status","text":"<pre><code># Run the script\npython mcp_interactive.py\n\n# Select option 2 to start server\nSelect option (1-9): 2\n\n# Wait for confirmation\n\u2713 Server started successfully (PID: 12345)\n\n# Select option 1 to check status\nSelect option (1-9): 1\n\n# View status information\n\u2713 Server is RUNNING (PID: 12345)\nOllama Connection: \u2713 Connected\nAvailable Models: 21\n</code></pre>"},{"location":"mcp_interactive/#example-2-configure-custom-ollama-host","title":"Example 2: Configure Custom Ollama Host","text":"<pre><code># Select option 6 (Manage environment variables)\nSelect option (1-9): 6\n\n# Select option 2 (Add/Update environment variable)\nSelect option (1-5): 2\n\n# Enter variable name\nEnter variable name: OLLAMA_HOST\n\n# Enter value\nEnter value for OLLAMA_HOST: http://ai:11434/\n\n\u2713 Set OLLAMA_HOST = http://ai:11434/\n</code></pre>"},{"location":"mcp_interactive/#example-3-execute-chat-command","title":"Example 3: Execute Chat Command","text":"<pre><code># Select option 8 (Run MCP command)\nSelect option (1-9): 8\n\n# Select ollama_chat\nSelect command number: 2\n\n# Enter model name\nmodel (string) [REQUIRED]\nEnter value: llava:latest\n\n# Enter message\nmessages (array) [REQUIRED]\nEnter your message: Explain what MCP is\n\n# Select output format\nSelect format (1 or 2, default: 1): 2\n\n# View result in Markdown format\nRESULT:\nMCP (Model Context Protocol) is a standardized protocol...\n\n\u2713 Command executed successfully.\n</code></pre>"},{"location":"mcp_interactive/#example-4-view-logs","title":"Example 4: View Logs","text":"<pre><code># Select option 4 (View server logs)\nSelect option (1-9): 4\n\n# View log content\nLog file: C:\\myCode\\gitHub\\mcp-ollama-python\\logs\\mcp_ollama_server.log\n\nLog content:\nStarting Ollama MCP Server...\nServer started successfully!\nWaiting for MCP client connections...\n\nFile Information:\n  Log file size: 156 bytes\n  Error log file size: 0 bytes\n</code></pre>"},{"location":"mcp_interactive/#technical-details","title":"Technical Details","text":""},{"location":"mcp_interactive/#process-management","title":"Process Management","text":"<p>PID Validation: - Uses <code>psutil.Process()</code> to verify process existence - Checks command line for <code>mcp_ollama_python</code> to confirm identity - Automatically cleans up stale PID files</p> <p>Process Creation: <pre><code># Windows-specific flags\ncreationflags = subprocess.CREATE_NEW_PROCESS_GROUP | subprocess.CREATE_NO_WINDOW\n\n# Unix: start_new_session=True\nprocess = subprocess.Popen(\n    [sys.executable, \"-m\", \"mcp_ollama_python\"],\n    stdin=stdin_read,\n    stdout=log_file,\n    stderr=error_log_file,\n    start_new_session=True,\n    creationflags=creationflags  # Windows only\n)\n</code></pre></p>"},{"location":"mcp_interactive/#pipe-file-management","title":"Pipe File Management","text":"<p>Purpose: Enable graceful server shutdown by closing stdin</p> <p>Implementation: 1. Create pipe: <code>stdin_read, stdin_write = os.pipe()</code> 2. Pass read end to subprocess 3. Store write end in <code>tmp/.mcp_ollama_server_{pid}.pipe</code> 4. Close write end during shutdown to signal EOF</p>"},{"location":"mcp_interactive/#stale-file-cleanup","title":"Stale File Cleanup","text":"<p>Automatic Cleanup: - Runs on server start, stop, and status check - Removes pipe files for non-existent processes - Removes pipe files for processes that aren't MCP servers - Handles invalid filename formats gracefully</p>"},{"location":"mcp_interactive/#async-command-execution","title":"Async Command Execution","text":"<p>Architecture: <pre><code>async def execute_command():\n    ollama_client = OllamaClient()\n    server = OllamaMCPServer(ollama_client)\n\n    # Get tools and execute\n    tools_result = await server.handle_list_tools()\n    result = await server.handle_call_tool(tool_name, args)\n\n    # Cleanup\n    await ollama_client.client.aclose()\n\nasyncio.run(execute_command())\n</code></pre></p>"},{"location":"mcp_interactive/#cross-platform-compatibility","title":"Cross-Platform Compatibility","text":"<p>Windows: - Uses <code>psutil.Process.terminate()</code> for force shutdown - Requires <code>CREATE_NEW_PROCESS_GROUP</code> flag - No SIGKILL signal available</p> <p>Unix/Linux: - Uses <code>signal.SIGTERM</code> for graceful shutdown - Uses <code>signal.SIGKILL</code> for force shutdown - Standard signal handling</p>"},{"location":"mcp_interactive/#troubleshooting","title":"Troubleshooting","text":""},{"location":"mcp_interactive/#common-issues","title":"Common Issues","text":""},{"location":"mcp_interactive/#1-server-wont-start","title":"1. Server Won't Start","text":"<p>Symptoms: Server fails to start, error in logs</p> <p>Solutions: - Check if Ollama is running: <code>curl http://localhost:11434/api/tags</code> - Verify Python version: <code>python --version</code> (requires 3.10+) - Ensure Poetry dependencies are installed: <code>poetry install</code> - Check for port conflicts - Review error log: <code>logs/mcp_ollama_server_error.log</code> - Verify you're running with Poetry: <code>poetry run python scripts/mcp_interactive.py</code></p>"},{"location":"mcp_interactive/#2-stale-pid-file","title":"2. Stale PID File","text":"<p>Symptoms: Script says server is running but it's not</p> <p>Solutions: - The script automatically detects and cleans stale PID files - If issue persists, manually delete: <code>tmp/.mcp_ollama_server.pid</code> - Run status check (Option 1) to trigger cleanup</p>"},{"location":"mcp_interactive/#3-cannot-connect-to-ollama","title":"3. Cannot Connect to Ollama","text":"<p>Symptoms: \"Cannot connect\" error when checking status</p> <p>Solutions: - Verify Ollama is running: <code>ollama list</code> - Check OLLAMA_HOST environment variable - Test connection: <code>curl http://localhost:11434/api/tags</code> - Configure custom host via Option 6</p>"},{"location":"mcp_interactive/#4-permission-denied-on-windows","title":"4. Permission Denied on Windows","text":"<p>Symptoms: Cannot terminate process or delete files</p> <p>Solutions: - Run as Administrator - Close any file handles to log files - Wait a few seconds and try again - Check antivirus software isn't blocking</p>"},{"location":"mcp_interactive/#5-logs-are-empty","title":"5. Logs Are Empty","text":"<p>Symptoms: Log files exist but contain no content</p> <p>Solutions: - Server may still be starting (wait a few seconds) - Check file permissions on <code>logs/</code> directory - Verify server is actually running (Option 1) - Check error log for startup issues</p>"},{"location":"mcp_interactive/#6-command-execution-fails","title":"6. Command Execution Fails","text":"<p>Symptoms: MCP command returns error</p> <p>Solutions: - Verify model name is correct: Use Option 5 to list available tools - Check required parameters are provided - Ensure Ollama has the specified model: <code>ollama list</code> - Review command syntax in tool description</p>"},{"location":"mcp_interactive/#7-poetry-specific-issues","title":"7. Poetry-Specific Issues","text":"Symptom Cause Fix <code>poetry: command not found</code> Poetry is not installed or the binary is not on the PATH. Install Poetry (e.g. <code>curl -sSL https://install.python-poetry.org | python3 -</code>) and add <code>~/.local/bin</code> (or the install prefix) to your <code>PATH</code>. Verify with <code>poetry --version</code>. Python version mismatch (<code>3.10+ required</code>) The current interpreter is older than the version required by the project. Switch to a supported Python version (e.g., <code>pyenv install 3.12.0 &amp;&amp; pyenv local 3.12.0</code>) or update the <code>python</code> key in <code>pyproject.toml</code> and re-run <code>poetry env use &lt;python&gt;</code>. <code>poetry.lock</code> conflicts or stale lock file The lock file was generated with a different Poetry version or after changes to dependencies. Delete the lock file (<code>rm poetry.lock</code>), run <code>poetry lock</code> to regenerate it, and then <code>poetry install</code>. If the conflict is due to a specific dependency, adjust the version specifier in <code>pyproject.toml</code> before locking again. Virtual environment issues (env not created / missing packages) Poetry is unable to create or activate the virtual environment, often due to missing system packages or an incorrectly configured setting. Ensure build tools are installed (<code>sudo apt install build-essential libffi-dev python3-dev</code> on Debian/Ubuntu, or Visual C++ Build Tools on Windows). If the env exists but is corrupted, delete it (<code>poetry env remove &lt;name&gt;</code>), then run <code>poetry install</code> to recreate."},{"location":"mcp_interactive/#debug-mode","title":"Debug Mode","text":"<p>For detailed debugging:</p> <ol> <li>Check Process: <code>ps aux | grep mcp_ollama_python</code> (Unix) or Task Manager (Windows)</li> <li>Manual Cleanup: Delete files in <code>tmp/</code> directory</li> <li>View Raw Logs: Open log files directly in text editor</li> <li>Test Ollama: <code>curl http://localhost:11434/api/tags</code></li> </ol>"},{"location":"mcp_interactive/#getting-help","title":"Getting Help","text":"<p>If issues persist:</p> <ol> <li>Check server logs (Option 4)</li> <li>Verify Ollama connection (Option 1)</li> <li>Review environment variables (Option 7)</li> <li>Check GitHub issues for similar problems</li> <li>Provide log files when reporting issues</li> </ol>"},{"location":"mcp_interactive/#advanced-usage","title":"Advanced Usage","text":""},{"location":"mcp_interactive/#custom-ollama-host","title":"Custom Ollama Host","text":"<p>For remote Ollama servers:</p> <pre><code># Set via environment variable management (Option 6)\nOLLAMA_HOST=http://remote-server:11434/\n</code></pre>"},{"location":"mcp_interactive/#batch-operations","title":"Batch Operations","text":"<p>While the script is interactive, you can script operations:</p> <pre><code># Start server programmatically\npython -c \"from mcp_interactive import MCPInteractive; m = MCPInteractive(); m.start_server()\"\n</code></pre>"},{"location":"mcp_interactive/#integration-with-other-tools","title":"Integration with Other Tools","text":"<p>The MCP server can be used by: - Claude Desktop - Cline VSCode Extension - Custom MCP clients - API integrations</p>"},{"location":"mcp_interactive/#security-considerations","title":"Security Considerations","text":"<ul> <li>API Keys: Store sensitive keys in environment variables, not in code</li> <li>Network Access: Configure firewall rules for Ollama host</li> <li>Process Isolation: Server runs in separate process for security</li> <li>Log Files: May contain sensitive information, protect accordingly</li> </ul>"},{"location":"mcp_interactive/#performance-tips","title":"Performance Tips","text":"<ul> <li>Model Selection: Smaller models respond faster</li> <li>Log Rotation: Periodically clear old log files</li> <li>Resource Monitoring: Check system resources if server is slow</li> <li>Network Latency: Use local Ollama instance for best performance</li> </ul>"},{"location":"mcp_interactive/#license","title":"License","text":"<p>This script is part of the mcp-ollama-python project. See the main project LICENSE file for details.</p>"},{"location":"mcp_interactive/#contributing","title":"Contributing","text":"<p>Contributions are welcome! Please see the main project CONTRIBUTING.md for guidelines.</p> <p>Last Updated: December 2025 Version: 1.0 Maintained By: mcp-ollama-python contributors</p>"},{"location":"tools/","title":"Available Tools","text":"<p>The Ollama MCP server exposes 8 comprehensive tools for interacting with your local Ollama models.</p>"},{"location":"tools/#model-management","title":"Model Management","text":"Tool Description <code>ollama_list</code> List all available local models <code>ollama_show</code> Get detailed information about a specific model <code>ollama_pull</code> Download models from Ollama library <code>ollama_delete</code> Remove models from local storage"},{"location":"tools/#model-operations","title":"Model Operations","text":"Tool Description <code>ollama_ps</code> List currently running models <code>ollama_generate</code> Generate text completions <code>ollama_chat</code> Interactive chat with models (supports tools/functions) <code>ollama_embed</code> Generate embeddings for text"},{"location":"tools/#web-tools-ollama-cloud-planned","title":"Web Tools (Ollama Cloud \u2014 Planned)","text":"Tool Description <code>ollama_web_search</code> Search the web with customizable result limits <code>ollama_web_fetch</code> Fetch and parse web page content"},{"location":"tools/#output-formats","title":"Output Formats","text":"<p>Most tools accept a <code>format</code> parameter:</p> <ul> <li><code>json</code> (default) \u2014 Structured JSON output</li> <li><code>markdown</code> \u2014 Human-readable Markdown output</li> </ul>"},{"location":"tools/#quick-reference","title":"Quick Reference","text":"<p>List models: <pre><code>{ \"tool\": \"ollama_list\", \"arguments\": { \"format\": \"markdown\" } }\n</code></pre></p> <p>Chat with a model: <pre><code>{\n  \"tool\": \"ollama_chat\",\n  \"arguments\": {\n    \"model\": \"llama3.2:latest\",\n    \"messages\": [{ \"role\": \"user\", \"content\": \"Hello!\" }]\n  }\n}\n</code></pre></p> <p>Generate text: <pre><code>{\n  \"tool\": \"ollama_generate\",\n  \"arguments\": {\n    \"model\": \"llama3.1\",\n    \"prompt\": \"Explain quantum computing in simple terms\"\n  }\n}\n</code></pre></p> <p>Generate embeddings: <pre><code>{\n  \"tool\": \"ollama_embed\",\n  \"arguments\": {\n    \"model\": \"nomic-embed-text\",\n    \"input\": [\"Hello world\", \"Embeddings are great\"]\n  }\n}\n</code></pre></p>"},{"location":"vscode/","title":"VS Code Integration","text":""},{"location":"vscode/#mcp-configuration","title":"MCP Configuration","text":"<p>Add the Ollama MCP server to your VS Code MCP settings:</p> <pre><code>{\n  \"mcpServers\": {\n    \"ollama\": {\n      \"command\": \"py\",\n      \"args\": [\"-m\", \"mcp_ollama_python\"],\n      \"disabled\": false\n    }\n  }\n}\n</code></pre>"},{"location":"vscode/#usage-examples","title":"Usage Examples","text":""},{"location":"vscode/#chat-with-a-model","title":"Chat with a Model","text":"<pre><code>{\n  \"tool\": \"ollama_chat\",\n  \"arguments\": {\n    \"model\": \"llama3.2:latest\",\n    \"messages\": [\n      { \"role\": \"user\", \"content\": \"Explain quantum computing\" }\n    ]\n  }\n}\n</code></pre>"},{"location":"vscode/#generate-embeddings","title":"Generate Embeddings","text":"<pre><code>{\n  \"tool\": \"ollama_embed\",\n  \"arguments\": {\n    \"model\": \"nomic-embed-text\",\n    \"input\": [\"Hello world\", \"Embeddings are great\"]\n  }\n}\n</code></pre>"},{"location":"vscode/#list-available-models","title":"List Available Models","text":"<pre><code>{\n  \"tool\": \"ollama_list\",\n  \"arguments\": {\n    \"format\": \"markdown\"\n  }\n}\n</code></pre>"},{"location":"vscode/#related","title":"Related","text":"<ul> <li>Available Tools \u2014 Full list of MCP tools and their arguments</li> <li>Configuration \u2014 Environment variables and model configuration</li> </ul>"},{"location":"windsurf/","title":"Windsurf Integration","text":"<p>Windsurf is an AI-powered code editor that supports MCP servers. This guide provides complete setup instructions for integrating the Ollama MCP server with Windsurf.</p>"},{"location":"windsurf/#step-1-configure-mcp-server","title":"Step 1: Configure MCP Server","text":"<p>Add the Ollama MCP server to your Windsurf MCP configuration:</p> <p><code>%USERPROFILE%\\.codeium\\windsurf\\mcp_config.json</code> (Windows) <code>~/.codeium/windsurf/mcp_config.json</code> (macOS/Linux)</p> <pre><code>{\n  \"mcpServers\": {\n    \"ollama\": {\n      \"command\": \"py\",\n      \"args\": [\"-m\", \"mcp_ollama_python\"],\n      \"disabled\": false,\n      \"env\": {}\n    },\n    \"git\": {\n      \"command\": \"py\",\n      \"args\": [\"-m\", \"mcp_server_git\"],\n      \"disabled\": true,\n      \"env\": {}\n    }\n  }\n}\n</code></pre> <p>Alternative Configuration (with Poetry): <pre><code>{\n  \"mcpServers\": {\n    \"ollama\": {\n      \"command\": \"py\",\n      \"args\": [\"-m\", \"poetry\", \"run\", \"mcp-ollama-python\"],\n      \"cwd\": \"d:/path/to/mcp-ollama-python\",\n      \"disabled\": false,\n      \"env\": {}\n    }\n  }\n}\n</code></pre></p> <p>Configuration Options:</p> <ul> <li><code>command</code> \u2014 Python interpreter command (<code>py</code>, <code>python</code>, or <code>python3</code>)</li> <li><code>args</code> \u2014 Module execution arguments</li> <li><code>disabled</code> \u2014 Set to <code>false</code> to enable the server</li> <li><code>env</code> \u2014 Environment variables (e.g., <code>OLLAMA_HOST</code>)</li> </ul>"},{"location":"windsurf/#step-2-windsurf-tools-workflow","title":"Step 2: Windsurf Tools Workflow","text":"<p>Create a workflow file to give Windsurf quick access to your MCP tools:</p> <p><code>.windsurf/workflows/tools.md</code> <pre><code>---\ndescription: Quick reference for Windsurf MCP tools (mcp-ollama)\nauto_execution_mode: 2\n---\n\n# MCP Tools (mcp-ollama)\n\nAvailable tools exposed by the local `mcp-ollama-python` server:\n\n- **ollama_chat** \u2013 Interactive chat with models (multi-turn, tool-calling, structured outputs)\n- **ollama_list** \u2013 List installed models\n- **ollama_show** \u2013 Show details for a specific model\n- **ollama_generate** \u2013 Single-prompt text generation\n- **ollama_pull** \u2013 Pull a model from a registry\n- **ollama_delete** \u2013 Delete a local model\n- **ollama_ps** \u2013 List running models\n- **ollama_embed** \u2013 Create embeddings for input text\n- **ollama_execute** \u2013 Execute a system command via the server (utility/test)\n\n## How to list tools in Windsurf\n1) Open the command palette and run: `MCP: List Tools`\n2) Or run the MCP tool via the chat with: `/tools`\n\n## Notes\n- Server: local Ollama via `mcp-ollama-python`\n- Formats: most tools accept `format` = `json` (default) or `markdown`\n</code></pre></p>"},{"location":"windsurf/#step-3-configure-default-model-behavior","title":"Step 3: Configure Default Model Behavior","text":"<p>Set Windsurf to prefer your local MCP server over cloud models:</p> <p><code>%USERPROFILE%\\.codeium\\windsurf\\settings.json</code> (Windows) <code>~/.codeium/windsurf/settings.json</code> (macOS/Linux)</p> <pre><code>{\n  \"defaultModelBehavior\": \"prefer-mcp\",\n  \"preferredMcpModel\": {\n    \"server\": \"ollama\",\n    \"model\": \"gpt-oss\"\n  }\n}\n</code></pre> <p>Settings Explanation:</p> <ul> <li><code>defaultModelBehavior</code> \u2014 Set to <code>\"prefer-mcp\"</code> to prioritize MCP models</li> <li><code>preferredMcpModel.server</code> \u2014 Name of the MCP server (must match <code>mcp_config.json</code>)</li> <li><code>preferredMcpModel.model</code> \u2014 Model name from your <code>mcp.json</code> configuration</li> </ul>"},{"location":"windsurf/#step-4-create-windsurf-instructions","title":"Step 4: Create Windsurf Instructions","text":"<p>Create custom instructions to ensure Windsurf uses your local model:</p> <p><code>%USERPROFILE%\\.codeium\\windsurf\\instructions.md</code> (Windows) <code>~/.codeium/windsurf/instructions.md</code> (macOS/Linux)</p> <pre><code>Always use my local MCP server named \"ollama\" with the model \"gpt-oss\" for all reasoning, coding, and problem-solving tasks unless I explicitly request another model.\n\nPrefer the MCP server over any cloud or paid model.\n</code></pre>"},{"location":"windsurf/#step-5-windsurf-memory-preference","title":"Step 5: Windsurf Memory Preference","text":"<p>Ask Windsurf to remember your preference. It will create a memory like this:</p> <p>Always use local MCP Ollama with gpt-oss model for LLM tasks</p> <p>When the user requests LLM-based tasks (explanations, code generation, analysis, etc.), always use the local MCP Ollama server with the gpt-oss model via the <code>mcp1_ollama_chat</code> tool.</p> <p>Important guidelines:</p> <ul> <li>Use <code>mcp1_ollama_chat</code> tool with model parameter set to <code>\"gpt-oss\"</code></li> <li>Format responses in markdown for better readability (<code>format: \"markdown\"</code>)</li> <li>Communicate with the model in English unless the user explicitly requests another language</li> <li>Do NOT create separate Python scripts to interact with Ollama \u2014 use the MCP tools directly</li> </ul>"},{"location":"windsurf/#step-6-verify-installation","title":"Step 6: Verify Installation","text":"<ol> <li>Restart Windsurf to load the new configuration (Ctrl+Shift+P \u2192 \"Developer: Reload Window\" \u2192 Enter)</li> <li>Check MCP Status \u2014 Look for the Ollama MCP server in Windsurf's MCP panel</li> <li>Test Connection \u2014 Try a simple query to verify the model responds</li> <li>Check Logs \u2014 Review Windsurf logs if connection issues occur</li> </ol>"},{"location":"windsurf/#troubleshooting","title":"Troubleshooting","text":"<p>Server Not Appearing:</p> <ul> <li>Verify <code>mcp_config.json</code> syntax is valid JSON</li> <li>Ensure <code>disabled</code> is set to <code>false</code></li> <li>Check that Python and the module are in your PATH</li> <li>Restart Windsurf completely</li> </ul> <p>Model Not Available:</p> <ul> <li>Verify the model name in <code>settings.json</code> matches <code>mcp.json</code></li> <li>Ensure Ollama is running (<code>ollama serve</code>)</li> <li>Check that the model is pulled (<code>ollama pull gpt-oss</code>)</li> </ul> <p>Connection Errors:</p> <ul> <li>Verify <code>OLLAMA_HOST</code> environment variable if using custom host</li> <li>Check Ollama server logs for errors</li> <li>Ensure no firewall blocking localhost connections</li> </ul>"}]}